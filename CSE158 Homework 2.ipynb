{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "def parseDataFromURL(fname):\n",
    "  for l in urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "# Download from http://jmcauley.ucsd.edu/cse258/data/amazon/book_descriptions_50000.json\n",
    "data = list(parseData(\"data/amazon/book_descriptions_50000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "### Naive bayes to determine p(childrens book | mentions wizards and mentions witches) ###\n",
    "\n",
    "# p(childrens book)\n",
    "prior = [\"Children's Books\" in b['categories'] for b in data]\n",
    "prior = sum(prior) * 1.0 / len(prior)\n",
    "\n",
    "# p(isn't children's book)\n",
    "prior_neg = 1 - prior\n",
    "\n",
    "# p(mentions wizards | is childrens)\n",
    "p1 = ['wizard' in b['description'] for b in data if \"Children's Books\" in b['categories']]\n",
    "p1 = sum(p1) * 1.0 / len(p1)\n",
    "\n",
    "# p(mentions wizards | isn't childrens)\n",
    "p1_neg = ['wizard' in b['description'] for b in data if not (\"Children's Books\" in b['categories'])]\n",
    "p1_neg = sum(p1_neg) * 1.0 / len(p1_neg)\n",
    "\n",
    "# p(mentions witches | is childrens)\n",
    "p2 = ['witch' in b['description'] for b in data if \"Children's Books\" in b['categories']]\n",
    "p2 = sum(p2) * 1.0 / len(p2)\n",
    "\n",
    "# p(mentions witches | isn't childrens)\n",
    "p2_neg = ['witch' in b['description'] for b in data if not (\"Children's Books\" in b['categories'])]\n",
    "p2_neg = sum(p2_neg) * 1.0 / len(p2_neg)\n",
    "\n",
    "# Prediction\n",
    "\n",
    "score = prior * p1 * p2\n",
    "score_neg = prior_neg * p1_neg * p2_neg\n",
    "\n",
    "# Actual ('non-naive') probability\n",
    "\n",
    "p = [\"Children's Books\" in b['categories'] for b in data if 'witch' in b['description'] and 'wizard' in b['description']]\n",
    "p = sum(p) * 1.0 / len(p)\n",
    "\n",
    "### Logistic Regression -- \"Judging a book by its cover\"\n",
    "\n",
    "print(\"Reading data...\")\n",
    "# Download from http://jmcauley.ucsd.edu/cse255/data/amazon/book_images_5000.json\n",
    "data = list(parseData(\"data/amazon/book_images_5000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "X = [b['image_feature'] for b in data]\n",
    "y = [\"Children's Books\" in b['categories'] for b in data]\n",
    "\n",
    "X_train = X[:2500]\n",
    "y_train = y[:2500]\n",
    "\n",
    "X_test = X[2500:]\n",
    "y_test = y[2500:]\n",
    "\n",
    "# Create a support vector classifier object, with regularization parameter C = 1000\n",
    "# clf = svm.SVC(C=1000, kernel='linear')\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# train_predictions = clf.predict(X_train)\n",
    "# test_predictions = clf.predict(X_test)\n",
    "\n",
    "# Logistic regression classifier\n",
    "mod = linear_model.LogisticRegression(C=1.0)\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = mod.predict(X_train)\n",
    "test_predictions = mod.predict(X_test)\n",
    "\n",
    "\n",
    "### Diagnostics\n",
    "\n",
    "# From https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data\n",
    "f = open(\"5year.arff\", 'r')\n",
    "\n",
    "# Reading in data\n",
    "while not '@data' in f.readline():\n",
    "    pass\n",
    "\n",
    "dataset = []\n",
    "for l in f:\n",
    "    if '?' in l: # Missing entry\n",
    "        continue\n",
    "    l = l.split(',')\n",
    "    values = [1] + [float(x) for x in l]\n",
    "    values[-1] = values[-1] > 0 # Convert to bool\n",
    "    dataset.append(values)\n",
    "\n",
    "# Data setup\n",
    "X = [d[:-1] for d in dataset]\n",
    "y = [d[-1] for d in dataset]\n",
    "\n",
    "# Fit model\n",
    "mod = linear_model.LogisticRegression(C=1.0)\n",
    "mod.fit(X,y)\n",
    "\n",
    "pred = mod.predict(X)\n",
    "\n",
    "# How many positive predictions?\n",
    "sum(pred)\n",
    "\n",
    "# Balanced model\n",
    "mod = linear_model.LogisticRegression(C=1.0, class_weight='balanced')\n",
    "mod.fit(X,y)\n",
    "\n",
    "pred = mod.predict(X)\n",
    "\n",
    "# How many positive predictions?\n",
    "sum(pred)\n",
    "\n",
    "# Train/validation/test splits\n",
    "\n",
    "# Shuffle the data\n",
    "Xy = list(zip(X,y))\n",
    "random.shuffle(Xy)\n",
    "\n",
    "X = [d[0] for d in Xy]\n",
    "y = [d[1] for d in Xy]\n",
    "\n",
    "N = len(y)\n",
    "\n",
    "Ntrain = 1000\n",
    "Nvalid = 1000\n",
    "Ntest = 1031\n",
    "\n",
    "Xtrain = X[:Ntrain]\n",
    "Xvalid = X[Ntrain:Ntrain+Nvalid]\n",
    "Xtest = X[Ntrain+Nvalid:]\n",
    "\n",
    "ytrain = y[:Ntrain]\n",
    "yvalid = y[Ntrain:Ntrain+Nvalid]\n",
    "ytest = y[Ntrain+Nvalid:]\n",
    "\n",
    "mod.fit(Xtrain, ytrain)\n",
    "\n",
    "pred = mod.predict(Xtest)\n",
    "\n",
    "correct = pred == ytest\n",
    "\n",
    "# True positives, false positives, etc.\n",
    "\n",
    "TP_ = numpy.logical_and(pred, ytest)\n",
    "FP_ = numpy.logical_and(pred, numpy.logical_not(ytest))\n",
    "TN_ = numpy.logical_and(numpy.logical_not(pred), numpy.logical_not(ytest))\n",
    "FN_ = numpy.logical_and(numpy.logical_not(pred), ytest)\n",
    "\n",
    "TP = sum(TP_)\n",
    "FP = sum(FP_)\n",
    "TN = sum(TN_)\n",
    "FN = sum(FN_)\n",
    "\n",
    "# accuracy\n",
    "sum(correct) / len(correct)\n",
    "(TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "# BER\n",
    "1 - 0.5 * (TP / (TP + FN) + TN / (TN + FP))\n",
    "\n",
    "# Ranking\n",
    "\n",
    "scores = mod.decision_function(Xtest)\n",
    "\n",
    "scores_labels = list(zip(scores, ytest))\n",
    "scores_labels.sort(reverse = True)\n",
    "\n",
    "sortedlabels = [x[1] for x in scores_labels]\n",
    "\n",
    "# precision / recall\n",
    "retrieved = sum(pred)\n",
    "relevant = sum(ytest)\n",
    "intersection = sum([y and p for y,p in zip(ytest,pred)])\n",
    "\n",
    "precision = intersection / retrieved\n",
    "recall = intersection / relevant\n",
    "\n",
    "# precision at 10\n",
    "sum(sortedlabels[:10]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parses the beer data\n",
    "parsed_beer = list(parseData(\"beer_50000.json\"))\n",
    "\n",
    "#Uses list comprehension to create lists of all the variables used for the predictive model\n",
    "cat = [d['beer/style'] for d in parsed_beer]\n",
    "abv = [d['beer/ABV'] for d in parsed_beer]\n",
    "aro = [d['review/aroma'] for d in parsed_beer]\n",
    "appear = [d['review/appearance'] for d in parsed_beer]\n",
    "pal = [d['review/palate'] for d in parsed_beer]\n",
    "tas = [d['review/taste'] for d in parsed_beer]\n",
    "over = [d['review/overall'] for d in parsed_beer]\n",
    "review_len = [len(d['review/text']) for d in parsed_beer]\n",
    "stan = [x/max(review_len) for x in review_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through all beer categories to see how many times each category appears\n",
    "categoryCounts = {}\n",
    "for d in parsed_beer:\n",
    "    if d['beer/style'] not in categoryCounts.keys():\n",
    "        categoryCounts[d['beer/style']] = 0\n",
    "    categoryCounts[d['beer/style']] += 1\n",
    "\n",
    "#Stores a dictionary of the indexes for all beer category that have over 1000 appearances\n",
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "catID = dict(zip(list(categories),range(len(categories))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the dataset into 50% train and 50% test\n",
    "x_train,x_test,y_train,y_test = train_test_split(np.array([cat,aro,appear,pal,tas,over,stan]).T,abv,test_size=0.5)\n",
    "\n",
    "#Creates empty matrices to allow for the one hot encoding values only\n",
    "train_matrix = np.zeros((25000,13))\n",
    "test_matrix = np.zeros((25000,13))\n",
    "\n",
    "#Converts the y_train and y_test values into binary scores for yes if over 7 and no if less then 7\n",
    "train_binary = [1 if x > 7 else 0 for x in y_train]\n",
    "test_binary = [1 if x > 7 else 0 for x in y_test]\n",
    "\n",
    "#One hot encodes the train and test matricies\n",
    "for x in range(len(x_train)):\n",
    "    if x_train[x][0] in catID.keys():\n",
    "        train_matrix[x][catID[x_train[x][0]]] = 1\n",
    "    if x_test[x][0] in catID.keys():\n",
    "        test_matrix[x][catID[x_test[x][0]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Logistic Regression model and stores prediction values\n",
    "clf = LogisticRegression(C=10,class_weight='balanced').fit(train_matrix, train_binary)\n",
    "pred = clf.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BER': 0.1633129177114595, 'Accuracy': 0.84652}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "TP_ = np.logical_and(pred, test_binary)\n",
    "FP_ = np.logical_and(pred, np.logical_not(test_binary))\n",
    "TN_ = np.logical_and(np.logical_not(pred), np.logical_not(test_binary))\n",
    "FN_ = np.logical_and(np.logical_not(pred), test_binary)\n",
    "\n",
    "#Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "TP = sum(TP_)\n",
    "FP = sum(FP_)\n",
    "TN = sum(TN_)\n",
    "FN = sum(FN_)\n",
    "\n",
    "#Converts To Rates\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(TN+FP)\n",
    "FNR = FN/(FN+TP)\n",
    "\n",
    "# accuracy\n",
    "#sum(correct) / len(correct)\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "#Calculates Balanced Error Rate\n",
    "ber = 1 - 0.5 * (TP / (TP + FN) + TN / (TN + FP))\n",
    "{'BER':ber, 'Accuracy':accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty matrices to allow for the one hot encoding and the rating and review length values\n",
    "train_matrix2 = np.zeros((25000,19))\n",
    "test_matrix2 = np.zeros((25000,19))\n",
    "\n",
    "#Populates the empty matricies with the proper values\n",
    "for x in range(len(x_train)):\n",
    "    if x_train[x][0] in catID.keys():\n",
    "        train_matrix2[x][catID[x_train[x][0]]] = 1\n",
    "    if x_test[x][0] in catID.keys():\n",
    "        test_matrix2[x][catID[x_test[x][0]]] = 1\n",
    "    \n",
    "    train_matrix2[x][13] = x_train[x][1]\n",
    "    train_matrix2[x][14] = x_train[x][2]\n",
    "    train_matrix2[x][15] = x_train[x][3]\n",
    "    train_matrix2[x][16] = x_train[x][4]\n",
    "    train_matrix2[x][17] = x_train[x][5]\n",
    "    train_matrix2[x][18] = x_train[x][6]\n",
    "    \n",
    "    test_matrix2[x][13] = x_test[x][1]\n",
    "    test_matrix2[x][14] = x_test[x][2]\n",
    "    test_matrix2[x][15] = x_test[x][3]\n",
    "    test_matrix2[x][16] = x_test[x][4]\n",
    "    test_matrix2[x][17] = x_test[x][5]\n",
    "    test_matrix2[x][18] = x_test[x][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Logistic Regression model and stores prediction values\n",
    "clf2 = LogisticRegression(C=10,class_weight='balanced',max_iter=500).fit(train_matrix2, train_binary)\n",
    "pred2 = clf2.predict(test_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BER': 0.1462271616105275, 'Accuracy': 0.85908}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "TP_2 = np.logical_and(pred2, test_binary)\n",
    "FP_2 = np.logical_and(pred2, np.logical_not(test_binary))\n",
    "TN_2 = np.logical_and(np.logical_not(pred2), np.logical_not(test_binary))\n",
    "FN_2 = np.logical_and(np.logical_not(pred2), test_binary)\n",
    "\n",
    "#Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "TP2 = sum(TP_2)\n",
    "FP2 = sum(FP_2)\n",
    "TN2 = sum(TN_2)\n",
    "FN2 = sum(FN_2)\n",
    "\n",
    "#Converts To Rates\n",
    "TPR2 = TP2/(TP2+FN2)\n",
    "FPR2 = FP2/(FP2+TN2)\n",
    "TNR2 = TN2/(TN2+FP2)\n",
    "FNR2 = FN2/(FN2+TP2)\n",
    "\n",
    "# accuracy\n",
    "#sum(correct) / len(correct)\n",
    "accuracy2 = (TP2 + TN2) / (TP2 + FP2 + TN2 + FN2)\n",
    "\n",
    "#Calculates Balanced Error Rate\n",
    "ber2 = 1 - 0.5 * (TP2 / (TP2 + FN2) + TN2 / (TN2 + FP2))\n",
    "{'BER':ber2, 'Accuracy':accuracy2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeps the splits as used in problems 1 and 2 but further splits the test sets into 50/50 so one half validation\n",
    "val_x,test_x,val_y,test_y=train_test_split(test_matrix2,test_binary,test_size=0.5)\n",
    "\n",
    "c_values = [0.000001, 0.00001, 0.0001, 0.001]\n",
    "models = [[train_matrix2,train_binary],[val_x,val_y],[test_x,test_y]]\n",
    "berDict = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Runs through all 4 C values and computes the BER of Train, Validation, and Test datasets for each C\n",
    "for x in c_values:\n",
    "    lr = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(train_matrix2, train_binary)\n",
    "    for y in range(3):\n",
    "        predC = lr.predict(models[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_C = np.logical_and(predC, models[y][1])\n",
    "        FP_C = np.logical_and(predC, np.logical_not(models[y][1]))\n",
    "        TN_C = np.logical_and(np.logical_not(predC), np.logical_not(models[y][1]))\n",
    "        FN_C = np.logical_and(np.logical_not(predC), models[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPC = sum(TP_C)\n",
    "        FPC = sum(FP_C)\n",
    "        TNC = sum(TN_C)\n",
    "        FNC = sum(FN_C)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPRC = TPC/(TPC+FNC)\n",
    "        FPRC = FPC/(FPC+TNC)\n",
    "        TNRC = TNC/(TNC+FPC)\n",
    "        FNRC = FNC/(FNC+TPC)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berC = 1 - 0.5 * (TPC / (TPC + FNC) + TNC / (TNC + FPC))\n",
    "        berDict[x].append(berC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all C values BER from Left to Right is Train, Validation, then Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1e-06: [0.3154150591598772, 0.32107064332098845, 0.3180212259300028],\n",
       " 1e-05: [0.31394380247075937, 0.3198286461623965, 0.31531422654950614],\n",
       " 0.0001: [0.2917197871624866, 0.29762025648805146, 0.293989311691386],\n",
       " 0.001: [0.19131646113964484, 0.19917168810274344, 0.1905613674752622]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('For all C values BER from Left to Right is Train, Validation, then Test')\n",
    "berDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the BER values reported on the Train, Validation, and Test datasets accounting for the 4 different C values, I believe the best model is the one that utilizes the 0.0001 C value. I believe this is the case, because it had the lowest and most consistent BER values across all three datasets. When determining which model is best, in terms of generalization, it is important that the validation BER is as low as the Train BER. This is because if the validation BER was lower, then the model would be overfitting and the model would not generalize for data that is different from that used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the matrix with all features into a DataFrame and drops the review length column\n",
    "dfTMOR = pd.DataFrame(train_matrix2).drop(columns=[18])\n",
    "dfVXOR = pd.DataFrame(val_x).drop(columns=[18])\n",
    "dfTXOR = pd.DataFrame(test_x).drop(columns=[18])\n",
    "\n",
    "#Converts the matrix with all features into a DataFrame and drops the ratings columns\n",
    "dfTMOS = pd.DataFrame(train_matrix2).drop(columns=[13,14,15,16,17])\n",
    "dfVXOS = pd.DataFrame(val_x).drop(columns=[13,14,15,16,17])\n",
    "dfTXOS = pd.DataFrame(test_x).drop(columns=[13,14,15,16,17])\n",
    "\n",
    "#Converts the matrix with all features into a DataFrame and drops the one-hot encoded category columns\n",
    "dfTMRS = pd.DataFrame(train_matrix2).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "dfVXRS = pd.DataFrame(val_x).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "dfTXRS = pd.DataFrame(test_x).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOR = [[dfTMOR,train_binary],[dfVXOR,val_y],[dfTXOR,test_y]]\n",
    "berDictOR = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with review length dropped\n",
    "for x in c_values:\n",
    "    lrOR = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMOR, train_binary)\n",
    "    for y in range(3):\n",
    "        predOR = lrOR.predict(modelOR[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_OR = np.logical_and(predOR, modelOR[y][1])\n",
    "        FP_OR = np.logical_and(predOR, np.logical_not(modelOR[y][1]))\n",
    "        TN_OR = np.logical_and(np.logical_not(predOR), np.logical_not(modelOR[y][1]))\n",
    "        FN_OR = np.logical_and(np.logical_not(predOR), modelOR[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPOR = sum(TP_OR)\n",
    "        FPOR = sum(FP_OR)\n",
    "        TNOR = sum(TN_OR)\n",
    "        FNOR = sum(FN_OR)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPROR = TPOR/(TPOR+FNOR)\n",
    "        FPROR = FPOR/(FPOR+TNOR)\n",
    "        TNROR = TNOR/(TNOR+FPOR)\n",
    "        FNROR = FNOR/(FNOR+TPOR)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berOR = 1 - 0.5 * (TPOR / (TPOR + FNOR) + TNOR / (TNOR + FPOR))\n",
    "        berDictOR[x].append(berOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOS = [[dfTMOS,train_binary],[dfVXOS,val_y],[dfTXOS,test_y]]\n",
    "berDictOS = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with ratings dropped\n",
    "for x in c_values:\n",
    "    lrOS = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMOS, train_binary)\n",
    "    for y in range(3):\n",
    "        predOS = lrOS.predict(modelOS[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_OS = np.logical_and(predOS, modelOS[y][1])\n",
    "        FP_OS = np.logical_and(predOS, np.logical_not(modelOS[y][1]))\n",
    "        TN_OS = np.logical_and(np.logical_not(predOS), np.logical_not(modelOS[y][1]))\n",
    "        FN_OS = np.logical_and(np.logical_not(predOS), modelOS[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPOS = sum(TP_OS)\n",
    "        FPOS = sum(FP_OS)\n",
    "        TNOS = sum(TN_OS)\n",
    "        FNOS = sum(FN_OS)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPROS = TPOS/(TPOS+FNOS)\n",
    "        FPROS = FPOS/(FPOS+TNOS)\n",
    "        TNROS = TNOS/(TNOS+FPOS)\n",
    "        FNROS = FNOS/(FNOS+TPOS)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berOS = 1 - 0.5 * (TPOS / (TPOS + FNOS) + TNOS / (TNOS + FPOS))\n",
    "        berDictOS[x].append(berOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRS = [[dfTMRS,train_binary],[dfVXRS,val_y],[dfTXRS,test_y]]\n",
    "berDictRS = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with one-hot encoded categories dropped\n",
    "for x in c_values:\n",
    "    lrRS = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMRS, train_binary)\n",
    "    for y in range(3):\n",
    "        predRS = lrRS.predict(modelRS[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_RS = np.logical_and(predRS, modelRS[y][1])\n",
    "        FP_RS = np.logical_and(predRS, np.logical_not(modelRS[y][1]))\n",
    "        TN_RS = np.logical_and(np.logical_not(predRS), np.logical_not(modelRS[y][1]))\n",
    "        FN_RS = np.logical_and(np.logical_not(predRS), modelRS[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPRS = sum(TP_RS)\n",
    "        FPRS = sum(FP_RS)\n",
    "        TNRS = sum(TN_RS)\n",
    "        FNRS = sum(FN_RS)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPRRS = TPRS/(TPRS+FNRS)\n",
    "        FPRRS = FPRS/(FPRS+TNRS)\n",
    "        TNRRS = TNRS/(TNRS+FPRS)\n",
    "        FNRRS = FNRS/(FNRS+TPRS)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berRS = 1 - 0.5 * (TPRS / (TPRS + FNRS) + TNRS / (TNRS + FPRS))\n",
    "        berDictRS[x].append(berRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all C values BER from Left to Right is Train, Validation, then Test\n",
      "{'One-Hot and Rating BER': {1e-06: [0.31539453642821536,\n",
      "                                    0.32131727831837376,\n",
      "                                    0.31790009025815313],\n",
      "                            1e-05: [0.31418498154857866,\n",
      "                                    0.32015015387616397,\n",
      "                                    0.31539946350825154],\n",
      "                            0.0001: [0.2915384783789452,\n",
      "                                     0.2980386537664397,\n",
      "                                     0.2941597856088767],\n",
      "                            0.001: [0.191522215834514,\n",
      "                                    0.199878567822541,\n",
      "                                    0.1905452040604143]},\n",
      " 'One-Hot and Review Length BER': {1e-06: [0.31539453642821536,\n",
      "                                           0.32131727831837376,\n",
      "                                           0.31790009025815313],\n",
      "                                   1e-05: [0.31418498154857866,\n",
      "                                           0.32015015387616397,\n",
      "                                           0.31539946350825154],\n",
      "                                   0.0001: [0.2915384783789452,\n",
      "                                            0.2980386537664397,\n",
      "                                            0.2941597856088767],\n",
      "                                   0.001: [0.191522215834514,\n",
      "                                           0.199878567822541,\n",
      "                                           0.1905452040604143]},\n",
      " 'Rating and Review Length BER': {1e-06: [0.3380789529488818,\n",
      "                                          0.3424664415392493,\n",
      "                                          0.3386856764186108],\n",
      "                                  1e-05: [0.33756482990083403,\n",
      "                                          0.3419841799685982,\n",
      "                                          0.33826935927401247],\n",
      "                                  0.0001: [0.33373459743584577,\n",
      "                                           0.3379190364357233,\n",
      "                                           0.33269118236124817],\n",
      "                                  0.001: [0.32091396142848094,\n",
      "                                          0.3229180266442959,\n",
      "                                          0.31648762879466363]}}\n"
     ]
    }
   ],
   "source": [
    "print('For all C values BER from Left to Right is Train, Validation, then Test')\n",
    "pprint.pprint({'One-Hot and Rating BER': berDictOR,'One-Hot and Review Length BER': berDictOR,\\\n",
    " 'Rating and Review Length BER': berDictRS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty sets to store the nodes and edges in a graph\n",
    "edges = set()\n",
    "nodes = set()\n",
    "\n",
    "#Reads each line of the egonet.txt file to populate the edges and nodes set\n",
    "with open(\"egonet.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        coord_x, coord_y = line.rstrip(\"\\n\").split(\" \")\n",
    "        coord_x,coord_y = int(coord_x),int(coord_y)\n",
    "        edges.add((coord_x,coord_y))\n",
    "        edges.add((coord_y,coord_x))\n",
    "        nodes.add(coord_x)\n",
    "        nodes.add(coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afong\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plots a graph of each edge and node to display a visual of all connected components\n",
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of connected components\n",
    "num_connect = len(list(nx.connected_components(G)))\n",
    "\n",
    "#Finds the number of nodes in the largest connected component\n",
    "num_nodes = len(max(list(nx.connected_components(G))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Connected Components': 3, 'Nodes In Largest Connected Component': 40}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'Connected Components': num_connect,'Nodes In Largest Connected Component': num_nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "# Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorts a list of all the node ID in the largest connected component\n",
    "largest = sorted(list(max(list(nx.connected_components(G)))))\n",
    "\n",
    "#Splits nodes in largest connected component between small and large ID\n",
    "lowID = largest[:len(largest)//2]\n",
    "highID = largest[len(largest)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4224058769513316"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates the normalized cut of the small and large ID values split early\n",
    "#Since the built in networkx normalized cut function is used I needed to divide the answer by two to represent\n",
    "#the 1/|C| constant that is multiplied in the normalized cut function shown in class\n",
    "#C is two in this case because there are two communities caused by the cut of the nodes in the largest connected\n",
    "#component into small and large ID\n",
    "#The built in networkx normalized cut function does not multiply its result by the 1/|C| constant multiplier\n",
    "#because it is meant to be used for directed graphs, but the graph here is undirected\n",
    "nx.normalized_cut_size(G,lowID,highID)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cut(G,S,T,weight=None):\n",
    "    e = nx.edge_boundary(G,S,T,data=weight,default=1)\n",
    "    nce = sum(weight for u,v,weight in e)\n",
    "    vs = sum(d for v,d in G.degree(S,weight=weight))\n",
    "    vt = sum(d for v,d in G.degree(T, weight=weight))\n",
    "    return (1/2)*nce*((1/vs)+(1/vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-3535a3dd8c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mhighTemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mhighTemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowTemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcurr_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalized_cut_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlowTemp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhighTemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m#curr_cost = normalized_cut(G,lowTemp,highTemp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcurr_cost\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcut_cost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\cuts.py\u001b[0m in \u001b[0;36mnormalized_cut_size\u001b[1;34m(G, S, T, weight)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mT\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0mnum_cut_edges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcut_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[0mvolume_S\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0mvolume_T\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\cuts.py\u001b[0m in \u001b[0;36mcut_size\u001b[1;34m(G, S, T, weight)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_boundary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\boundary.py\u001b[0m in \u001b[0;36medge_boundary\u001b[1;34m(G, nbunch1, nbunch2, data, keys, default)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;31m# If `nbunch2` is not provided, then it is assumed to be the set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;31m# complement of `nbunch1`. For the sake of efficiency, this is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\classes\\reportviews.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, nbunch, data, default)\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnbunch\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbunch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbunch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\classes\\reportviews.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, viewer, nbunch, data, default)\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_nbrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adjdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m             \u001b[0mnbunch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbunch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbunch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_nbrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adjdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnbunch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nbunch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnbunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\classes\\graph.py\u001b[0m in \u001b[0;36mbunch_iter\u001b[1;34m(nlist, adj)\u001b[0m\n\u001b[0;32m   1908\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1910\u001b[1;33m                             \u001b[1;32myield\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1911\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "first_loop = False\n",
    "second_loop = False\n",
    "cut_cost = 0\n",
    "curr_cost = 1\n",
    "num = 0\n",
    "low = lowID.copy()\n",
    "high = highID.copy()\n",
    "while cut_cost < curr_cost:\n",
    "    change = False\n",
    "    for x in low:\n",
    "        lowTemp = low.copy()\n",
    "        highTemp = high.copy()\n",
    "        highTemp.append(lowTemp.remove(x))\n",
    "        curr_cost = nx.normalized_cut_size(G,lowTemp,highTemp)/2\n",
    "        #curr_cost = normalized_cut(G,lowTemp,highTemp)\n",
    "        if curr_cost < cut_cost:\n",
    "            cut_cost = curr_cost\n",
    "            num = x\n",
    "            first_loop = True\n",
    "            second_loop = False\n",
    "            change = True\n",
    "    for x in high:\n",
    "        lowTemp = low.copy()\n",
    "        highTemp = high.copy()\n",
    "        lowTemp.append(highTemp.remove(x))\n",
    "        curr_cost = nx.normalized_cut_size(G,lowTemp,highTemp)/2\n",
    "        #curr_cost = normalized_cut(G,lowTemp,highTemp)\n",
    "        if curr_cost < cut_cost:\n",
    "            cut_cost = curr_cost\n",
    "            num = x\n",
    "            first_loop = False\n",
    "            second_loop = True\n",
    "            change = True\n",
    "    if change == True:\n",
    "        if first_loop == True:\n",
    "            high.append(low.remove(num))\n",
    "        else:\n",
    "            low.append(high.remove(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find all 3 and 4-cliques in the graph ###\n",
    "cliques3 = set()\n",
    "cliques4 = set()\n",
    "for n1 in nodes:\n",
    "  for n2 in nodes:\n",
    "    if not ((n1,n2) in edges): continue\n",
    "    for n3 in nodes:\n",
    "      if not ((n1,n3) in edges): continue\n",
    "      if not ((n2,n3) in edges): continue\n",
    "      clique = [n1,n2,n3]\n",
    "      clique.sort()\n",
    "      cliques3.add(tuple(clique))\n",
    "      for n4 in nodes:\n",
    "        if not ((n1,n4) in edges): continue\n",
    "        if not ((n2,n4) in edges): continue\n",
    "        if not ((n3,n4) in edges): continue\n",
    "        clique = [n1,n2,n3,n4]\n",
    "        clique.sort()\n",
    "        cliques4.add(tuple(clique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
