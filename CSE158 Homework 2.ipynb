{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in open(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parses the beer data\n",
    "parsed_beer = list(parseData(\"beer_50000.json\"))\n",
    "\n",
    "#Uses list comprehension to create lists of all the variables used for the predictive model\n",
    "cat = [d['beer/style'] for d in parsed_beer]\n",
    "abv = [d['beer/ABV'] for d in parsed_beer]\n",
    "aro = [d['review/aroma'] for d in parsed_beer]\n",
    "appear = [d['review/appearance'] for d in parsed_beer]\n",
    "pal = [d['review/palate'] for d in parsed_beer]\n",
    "tas = [d['review/taste'] for d in parsed_beer]\n",
    "over = [d['review/overall'] for d in parsed_beer]\n",
    "review_len = [len(d['review/text']) for d in parsed_beer]\n",
    "stan = [x/max(review_len) for x in review_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through all beer categories to see how many times each category appears\n",
    "categoryCounts = {}\n",
    "for d in parsed_beer:\n",
    "    if d['beer/style'] not in categoryCounts.keys():\n",
    "        categoryCounts[d['beer/style']] = 0\n",
    "    categoryCounts[d['beer/style']] += 1\n",
    "\n",
    "#Stores a dictionary of the indexes for all beer category that have over 1000 appearances\n",
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]\n",
    "catID = dict(zip(list(categories),range(len(categories))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the dataset into 50% train and 50% test\n",
    "x_train,x_test,y_train,y_test = train_test_split(np.array([cat,aro,appear,pal,tas,over,stan]).T,abv,test_size=0.5)\n",
    "\n",
    "#Creates empty matrices to allow for the one hot encoding values only\n",
    "train_matrix = np.zeros((25000,13))\n",
    "test_matrix = np.zeros((25000,13))\n",
    "\n",
    "#Converts the y_train and y_test values into binary scores for yes if over 7 and no if less then 7\n",
    "train_binary = [1 if x > 7 else 0 for x in y_train]\n",
    "test_binary = [1 if x > 7 else 0 for x in y_test]\n",
    "\n",
    "#One hot encodes the train and test matricies\n",
    "for x in range(len(x_train)):\n",
    "    if x_train[x][0] in catID.keys():\n",
    "        train_matrix[x][catID[x_train[x][0]]] = 1\n",
    "    if x_test[x][0] in catID.keys():\n",
    "        test_matrix[x][catID[x_test[x][0]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Logistic Regression model and stores prediction values\n",
    "clf = LogisticRegression(C=10,class_weight='balanced').fit(train_matrix, train_binary)\n",
    "pred = clf.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BER': 0.1605647234430716, 'Accuracy': 0.84924}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "TP_ = np.logical_and(pred, test_binary)\n",
    "FP_ = np.logical_and(pred, np.logical_not(test_binary))\n",
    "TN_ = np.logical_and(np.logical_not(pred), np.logical_not(test_binary))\n",
    "FN_ = np.logical_and(np.logical_not(pred), test_binary)\n",
    "\n",
    "#Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "TP = sum(TP_)\n",
    "FP = sum(FP_)\n",
    "TN = sum(TN_)\n",
    "FN = sum(FN_)\n",
    "\n",
    "#Converts To Rates\n",
    "TPR = TP/(TP+FN)\n",
    "FPR = FP/(FP+TN)\n",
    "TNR = TN/(TN+FP)\n",
    "FNR = FN/(FN+TP)\n",
    "\n",
    "# accuracy\n",
    "#sum(correct) / len(correct)\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "#Calculates Balanced Error Rate\n",
    "ber = 1 - 0.5 * (TP / (TP + FN) + TN / (TN + FP))\n",
    "{'BER':ber, 'Accuracy':accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty matrices to allow for the one hot encoding and the rating and review length values\n",
    "train_matrix2 = np.zeros((25000,19))\n",
    "test_matrix2 = np.zeros((25000,19))\n",
    "\n",
    "#Populates the empty matricies with the proper values\n",
    "for x in range(len(x_train)):\n",
    "    if x_train[x][0] in catID.keys():\n",
    "        train_matrix2[x][catID[x_train[x][0]]] = 1\n",
    "    if x_test[x][0] in catID.keys():\n",
    "        test_matrix2[x][catID[x_test[x][0]]] = 1\n",
    "    \n",
    "    train_matrix2[x][13] = x_train[x][1]\n",
    "    train_matrix2[x][14] = x_train[x][2]\n",
    "    train_matrix2[x][15] = x_train[x][3]\n",
    "    train_matrix2[x][16] = x_train[x][4]\n",
    "    train_matrix2[x][17] = x_train[x][5]\n",
    "    train_matrix2[x][18] = x_train[x][6]\n",
    "    \n",
    "    test_matrix2[x][13] = x_test[x][1]\n",
    "    test_matrix2[x][14] = x_test[x][2]\n",
    "    test_matrix2[x][15] = x_test[x][3]\n",
    "    test_matrix2[x][16] = x_test[x][4]\n",
    "    test_matrix2[x][17] = x_test[x][5]\n",
    "    test_matrix2[x][18] = x_test[x][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Logistic Regression model and stores prediction values\n",
    "clf2 = LogisticRegression(C=10,class_weight='balanced',max_iter=500).fit(train_matrix2, train_binary)\n",
    "pred2 = clf2.predict(test_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BER': 0.14121878798748533, 'Accuracy': 0.86368}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "TP_2 = np.logical_and(pred2, test_binary)\n",
    "FP_2 = np.logical_and(pred2, np.logical_not(test_binary))\n",
    "TN_2 = np.logical_and(np.logical_not(pred2), np.logical_not(test_binary))\n",
    "FN_2 = np.logical_and(np.logical_not(pred2), test_binary)\n",
    "\n",
    "#Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "TP2 = sum(TP_2)\n",
    "FP2 = sum(FP_2)\n",
    "TN2 = sum(TN_2)\n",
    "FN2 = sum(FN_2)\n",
    "\n",
    "#Converts To Rates\n",
    "TPR2 = TP2/(TP2+FN2)\n",
    "FPR2 = FP2/(FP2+TN2)\n",
    "TNR2 = TN2/(TN2+FP2)\n",
    "FNR2 = FN2/(FN2+TP2)\n",
    "\n",
    "# accuracy\n",
    "#sum(correct) / len(correct)\n",
    "accuracy2 = (TP2 + TN2) / (TP2 + FP2 + TN2 + FN2)\n",
    "\n",
    "#Calculates Balanced Error Rate\n",
    "ber2 = 1 - 0.5 * (TP2 / (TP2 + FN2) + TN2 / (TN2 + FP2))\n",
    "{'BER':ber2, 'Accuracy':accuracy2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeps the splits as used in problems 1 and 2 but further splits the test sets into 50/50 so one half validation\n",
    "val_x,test_x,val_y,test_y=train_test_split(test_matrix2,test_binary,test_size=0.5)\n",
    "\n",
    "c_values = [0.000001, 0.00001, 0.0001, 0.001]\n",
    "models = [[train_matrix2,train_binary],[val_x,val_y],[test_x,test_y]]\n",
    "berDict = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Runs through all 4 C values and computes the BER of Train, Validation, and Test datasets for each C\n",
    "for x in c_values:\n",
    "    lr = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(train_matrix2, train_binary)\n",
    "    for y in range(3):\n",
    "        predC = lr.predict(models[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_C = np.logical_and(predC, models[y][1])\n",
    "        FP_C = np.logical_and(predC, np.logical_not(models[y][1]))\n",
    "        TN_C = np.logical_and(np.logical_not(predC), np.logical_not(models[y][1]))\n",
    "        FN_C = np.logical_and(np.logical_not(predC), models[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPC = sum(TP_C)\n",
    "        FPC = sum(FP_C)\n",
    "        TNC = sum(TN_C)\n",
    "        FNC = sum(FN_C)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPRC = TPC/(TPC+FNC)\n",
    "        FPRC = FPC/(FPC+TNC)\n",
    "        TNRC = TNC/(TNC+FPC)\n",
    "        FNRC = FNC/(FNC+TPC)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berC = 1 - 0.5 * (TPC / (TPC + FNC) + TNC / (TNC + FPC))\n",
    "        berDict[x].append(berC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all C values BER from Left to Right is Train, Validation, then Test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1e-06: [0.3203153573998061, 0.3162886136366819, 0.3113981699051457],\n",
       " 1e-05: [0.3183763958193091, 0.3145667599185118, 0.30982077474227854],\n",
       " 0.0001: [0.29569671441750667, 0.29420936522019225, 0.28842378233508525],\n",
       " 0.001: [0.19729327285490306, 0.1879529242938356, 0.1885960761781582]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('For all C values BER from Left to Right is Train, Validation, then Test')\n",
    "berDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the BER values reported on the Train, Validation, and Test datasets accounting for the 4 different C values, I believe the best model is the one that utilizes the 0.001 C value. I believe this is the case, because it had the lowest BER values across all three datasets. When determining which model is best, in terms of generalization, it is important that the BER for both the validation and test datasets are low. This is because the train BER is meant to be lower, since the model is trained using the train data. So to check for overfitting it is important that unseen data like the validation and test sets are low as well. If it works well for unseen data then the model generalizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the matrix with all features into a DataFrame and drops the review length column\n",
    "dfTMOR = pd.DataFrame(train_matrix2).drop(columns=[18])\n",
    "dfVXOR = pd.DataFrame(val_x).drop(columns=[18])\n",
    "dfTXOR = pd.DataFrame(test_x).drop(columns=[18])\n",
    "\n",
    "#Converts the matrix with all features into a DataFrame and drops the ratings columns\n",
    "dfTMOS = pd.DataFrame(train_matrix2).drop(columns=[13,14,15,16,17])\n",
    "dfVXOS = pd.DataFrame(val_x).drop(columns=[13,14,15,16,17])\n",
    "dfTXOS = pd.DataFrame(test_x).drop(columns=[13,14,15,16,17])\n",
    "\n",
    "#Converts the matrix with all features into a DataFrame and drops the one-hot encoded category columns\n",
    "dfTMRS = pd.DataFrame(train_matrix2).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "dfVXRS = pd.DataFrame(val_x).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "dfTXRS = pd.DataFrame(test_x).drop(columns=[0,1,2,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOR = [[dfTMOR,train_binary],[dfVXOR,val_y],[dfTXOR,test_y]]\n",
    "berDictOR = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with review length dropped\n",
    "for x in c_values:\n",
    "    lrOR = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMOR, train_binary)\n",
    "    for y in range(3):\n",
    "        predOR = lrOR.predict(modelOR[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_OR = np.logical_and(predOR, modelOR[y][1])\n",
    "        FP_OR = np.logical_and(predOR, np.logical_not(modelOR[y][1]))\n",
    "        TN_OR = np.logical_and(np.logical_not(predOR), np.logical_not(modelOR[y][1]))\n",
    "        FN_OR = np.logical_and(np.logical_not(predOR), modelOR[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPOR = sum(TP_OR)\n",
    "        FPOR = sum(FP_OR)\n",
    "        TNOR = sum(TN_OR)\n",
    "        FNOR = sum(FN_OR)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPROR = TPOR/(TPOR+FNOR)\n",
    "        FPROR = FPOR/(FPOR+TNOR)\n",
    "        TNROR = TNOR/(TNOR+FPOR)\n",
    "        FNROR = FNOR/(FNOR+TPOR)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berOR = 1 - 0.5 * (TPOR / (TPOR + FNOR) + TNOR / (TNOR + FPOR))\n",
    "        berDictOR[x].append(berOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOS = [[dfTMOS,train_binary],[dfVXOS,val_y],[dfTXOS,test_y]]\n",
    "berDictOS = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with ratings dropped\n",
    "for x in c_values:\n",
    "    lrOS = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMOS, train_binary)\n",
    "    for y in range(3):\n",
    "        predOS = lrOS.predict(modelOS[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_OS = np.logical_and(predOS, modelOS[y][1])\n",
    "        FP_OS = np.logical_and(predOS, np.logical_not(modelOS[y][1]))\n",
    "        TN_OS = np.logical_and(np.logical_not(predOS), np.logical_not(modelOS[y][1]))\n",
    "        FN_OS = np.logical_and(np.logical_not(predOS), modelOS[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPOS = sum(TP_OS)\n",
    "        FPOS = sum(FP_OS)\n",
    "        TNOS = sum(TN_OS)\n",
    "        FNOS = sum(FN_OS)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPROS = TPOS/(TPOS+FNOS)\n",
    "        FPROS = FPOS/(FPOS+TNOS)\n",
    "        TNROS = TNOS/(TNOS+FPOS)\n",
    "        FNROS = FNOS/(FNOS+TPOS)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berOS = 1 - 0.5 * (TPOS / (TPOS + FNOS) + TNOS / (TNOS + FPOS))\n",
    "        berDictOS[x].append(berOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRS = [[dfTMRS,train_binary],[dfVXRS,val_y],[dfTXRS,test_y]]\n",
    "berDictRS = {0.000001:[], 0.00001:[], 0.0001:[], 0.001:[]}\n",
    "\n",
    "#Computes the BER of Train, Validation, and Test datasets for each C with one-hot encoded categories dropped\n",
    "for x in c_values:\n",
    "    lrRS = LogisticRegression(C=x,class_weight='balanced',max_iter=500).fit(dfTMRS, train_binary)\n",
    "    for y in range(3):\n",
    "        predRS = lrRS.predict(modelRS[y][0])\n",
    "\n",
    "        #Finds the True Positive, False Positive, True Negative, and False Negative array values\n",
    "        TP_RS = np.logical_and(predRS, modelRS[y][1])\n",
    "        FP_RS = np.logical_and(predRS, np.logical_not(modelRS[y][1]))\n",
    "        TN_RS = np.logical_and(np.logical_not(predRS), np.logical_not(modelRS[y][1]))\n",
    "        FN_RS = np.logical_and(np.logical_not(predRS), modelRS[y][1])\n",
    "\n",
    "        #Finds the number of True Positive, False Positive, True Negative, and False Negative\n",
    "        TPRS = sum(TP_RS)\n",
    "        FPRS = sum(FP_RS)\n",
    "        TNRS = sum(TN_RS)\n",
    "        FNRS = sum(FN_RS)\n",
    "\n",
    "        #Converts To Rates\n",
    "        TPRRS = TPRS/(TPRS+FNRS)\n",
    "        FPRRS = FPRS/(FPRS+TNRS)\n",
    "        TNRRS = TNRS/(TNRS+FPRS)\n",
    "        FNRRS = FNRS/(FNRS+TPRS)\n",
    "\n",
    "        #Calculates Balanced Error Rate\n",
    "        berRS = 1 - 0.5 * (TPRS / (TPRS + FNRS) + TNRS / (TNRS + FPRS))\n",
    "        berDictRS[x].append(berRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all C values BER from Left to Right is Train, Validation, then Test\n",
      "{'One-Hot and Rating BER': {1e-06: [0.3204606917467375,\n",
      "                                    0.31647350065943547,\n",
      "                                    0.31149246735243086],\n",
      "                            1e-05: [0.318591098670563,\n",
      "                                    0.3145667599185118,\n",
      "                                    0.309990669407586],\n",
      "                            0.0001: [0.29603364164122636,\n",
      "                                     0.29408693098457306,\n",
      "                                     0.2889436127466105],\n",
      "                            0.001: [0.19706865470575663,\n",
      "                                    0.1886084983569717,\n",
      "                                    0.18892651539414174]},\n",
      " 'One-Hot and Review Length BER': {1e-06: [0.3204606917467375,\n",
      "                                           0.31647350065943547,\n",
      "                                           0.31149246735243086],\n",
      "                                   1e-05: [0.318591098670563,\n",
      "                                           0.3145667599185118,\n",
      "                                           0.309990669407586],\n",
      "                                   0.0001: [0.29603364164122636,\n",
      "                                            0.29408693098457306,\n",
      "                                            0.2889436127466105],\n",
      "                                   0.001: [0.19706865470575663,\n",
      "                                           0.1886084983569717,\n",
      "                                           0.18892651539414174]},\n",
      " 'Rating and Review Length BER': {1e-06: [0.3409624284542454,\n",
      "                                          0.33930182211798665,\n",
      "                                          0.33470140408924687],\n",
      "                                  1e-05: [0.34081818723354085,\n",
      "                                          0.3389055841544355,\n",
      "                                          0.33448396254981116],\n",
      "                                  0.0001: [0.33655926457142404,\n",
      "                                           0.33477736739826525,\n",
      "                                           0.3293168939823381],\n",
      "                                  0.001: [0.32202618996692234,\n",
      "                                          0.3197622860541536,\n",
      "                                          0.31668041955505577]}}\n"
     ]
    }
   ],
   "source": [
    "print('For all C values BER from Left to Right is Train, Validation, then Test')\n",
    "pprint.pprint({'One-Hot and Rating BER': berDictOR,'One-Hot and Review Length BER': berDictOR,\\\n",
    " 'Rating and Review Length BER': berDictRS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates empty sets to store the nodes and edges in a graph\n",
    "edges = set()\n",
    "nodes = set()\n",
    "\n",
    "#Reads each line of the egonet.txt file to populate the edges and nodes set\n",
    "with open(\"egonet.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        coord_x, coord_y = line.rstrip(\"\\n\").split(\" \")\n",
    "        coord_x,coord_y = int(coord_x),int(coord_y)\n",
    "        edges.add((coord_x,coord_y))\n",
    "        edges.add((coord_y,coord_x))\n",
    "        nodes.add(coord_x)\n",
    "        nodes.add(coord_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afong\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:579: MatplotlibDeprecationWarning: \n",
      "The iterable function was deprecated in Matplotlib 3.1 and will be removed in 3.3. Use np.iterable instead.\n",
      "  if not cb.iterable(width):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plots a graph of each edge and node to display a visual of all connected components\n",
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of connected components\n",
    "num_connect = len(list(nx.connected_components(G)))\n",
    "\n",
    "#Finds the number of nodes in the largest connected component\n",
    "num_nodes = len(max(list(nx.connected_components(G))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Connected Components': 3, 'Nodes In Largest Connected Component': 40}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'Connected Components': num_connect,'Nodes In Largest Connected Component': num_nodes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorts a list of all the node ID in the largest connected component\n",
    "largest = sorted(list(max(list(nx.connected_components(G)))))\n",
    "\n",
    "#Splits nodes in largest connected component between small and large ID\n",
    "lowID = largest[:len(largest)//2]\n",
    "highID = largest[len(largest)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4224058769513316"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates the normalized cut of the small and large ID values split early\n",
    "#Since the built in networkx normalized cut function is used I needed to divide the answer by two to represent\n",
    "#the 1/|C| constant that is multiplied in the normalized cut function shown in class\n",
    "#C is two in this case because there are two communities caused by the cut of the nodes in the largest connected\n",
    "#component into small and large ID\n",
    "#The built in networkx normalized cut function does not multiply its result by the 1/|C| constant multiplier\n",
    "#because it is meant to be used for directed graphs, but the graph here is undirected\n",
    "nx.normalized_cut_size(G,lowID,highID)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates conditionals for the while loop and to check which list should move which element\n",
    "first_loop = False\n",
    "second_loop = False\n",
    "change = True\n",
    "\n",
    "#Variables to compare current cost with lowest cost\n",
    "cut_cost = 1\n",
    "curr_cost = 10\n",
    "\n",
    "#Stores current value that led to lowest cost and copies the two lists\n",
    "num = 0\n",
    "low = lowID.copy()\n",
    "high = highID.copy()\n",
    "\n",
    "#While loop to find lowest cost split\n",
    "while change == True:\n",
    "    change = False\n",
    "    \n",
    "    #Loop through low list trying each value and checking the cost\n",
    "    for x in low:\n",
    "        lowTemp = low.copy()\n",
    "        highTemp = high.copy()\n",
    "        lowTemp.remove(x)\n",
    "        highTemp.append(x)\n",
    "        curr_cost = nx.normalized_cut_size(G,lowTemp,highTemp)/2\n",
    "        \n",
    "        #Checks to see if current cost is less than lowest cost\n",
    "        if curr_cost < cut_cost:\n",
    "            cut_cost = curr_cost\n",
    "            num = x\n",
    "            first_loop = True\n",
    "            second_loop = False\n",
    "            change = True\n",
    "        \n",
    "        #Checks to see if current and lowest cost are the same\n",
    "        if curr_cost == cut_cost:\n",
    "            if num > x:    \n",
    "                num = x\n",
    "                first_loop = True\n",
    "                second_loop = False\n",
    "                change = True\n",
    "                \n",
    "    #Loop through high list trying each value and checking the cost\n",
    "    for y in high:\n",
    "        lowTemp = low.copy()\n",
    "        highTemp = high.copy()\n",
    "        highTemp.remove(y)\n",
    "        lowTemp.append(y)\n",
    "        curr_cost = nx.normalized_cut_size(G,lowTemp,highTemp)/2\n",
    "        \n",
    "        #Checks to see if current cost is less than lowest cost\n",
    "        if curr_cost < cut_cost:\n",
    "            cut_cost = curr_cost\n",
    "            num = y\n",
    "            first_loop = False\n",
    "            second_loop = True\n",
    "            change = True\n",
    "            \n",
    "        #Checks to see if current and lowest cost are the same\n",
    "        if curr_cost == cut_cost:\n",
    "            if num > y:    \n",
    "                num = y\n",
    "                first_loop = False\n",
    "                second_loop = True\n",
    "                change = True\n",
    "                \n",
    "    #Checks to see if there was a change in cost\n",
    "    if change == True:\n",
    "        if first_loop == True:\n",
    "            low.remove(num)\n",
    "            high.append(num)\n",
    "        else:\n",
    "            high.remove(num)\n",
    "            low.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09817045961624274"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[697,\n",
       " 703,\n",
       " 708,\n",
       " 713,\n",
       " 719,\n",
       " 745,\n",
       " 747,\n",
       " 753,\n",
       " 769,\n",
       " 772,\n",
       " 774,\n",
       " 798,\n",
       " 800,\n",
       " 803,\n",
       " 805,\n",
       " 810,\n",
       " 811,\n",
       " 819,\n",
       " 828,\n",
       " 823,\n",
       " 830,\n",
       " 840,\n",
       " 880,\n",
       " 890,\n",
       " 869,\n",
       " 856]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[825, 861, 863, 864, 876, 878, 882, 884, 886, 888, 889, 893, 729, 804]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
